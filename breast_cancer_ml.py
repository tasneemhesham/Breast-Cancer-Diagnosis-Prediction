# -*- coding: utf-8 -*-
"""Breast_Cancer_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WNB4gd9UIPXp3Vgh9njXW5LL2KMIHwM_

# Importing Necessary Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier


from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

df = pd.read_csv('data.csv')

"""# Preprocess the df
## Explore df and df Cleaning
"""

df.head()

df.shape

df.info()

df.duplicated().sum()

df.isna().sum()

df.nunique()

df = df.drop(['id','Unnamed: 32'], axis= 1)

df = df.rename(columns={'diagnosis' : 'target'})

df.target.replace({'M' : '1','B': '0'},inplace=True)

df.target = df.target.astype('float64')

df.head()

df.tail()

df.info()

df.describe().T

"""# Analysis & EDA"""

df.target.value_counts()

df['target'].value_counts().plot(kind='bar',edgecolor='black',color=['lightsteelblue','navajowhite'])
plt.title(" Target",fontsize=20)
plt.show()

cor = df.corr()
cor

plt.figure(figsize=(25,23))
sns.heatmap(cor, annot= True, linewidths= 0.3 ,linecolor = "black", fmt = ".2f")
plt.title('Correlation Heatmap')
plt.show()

threshold = 0.75
filtre = np.abs(cor["target"] > threshold)
corr_features = cor.columns[filtre].tolist()
plt.figure(figsize=(10,8))
sns.clustermap(df[corr_features].corr(), annot = True, fmt = ".2f")
plt.title("\nCorrelation Between Features with Threshold [0.75]\n",fontsize=20)
plt.show()

"""# **Model Building and Evaluation**

#Splitting the Data
"""

x= df.drop('target',axis=1)
y= df['target']

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.30,random_state=101)

"""# Standardizing the Data"""

s= StandardScaler()
x_train = s.fit_transform(x_train)
x_test = s.fit_transform(x_test)

"""### 1- Logistic Regression"""

log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)

log_accuracy = accuracy_score(y_test,log_reg.predict(x_test))
print(f'Accuracy: {log_accuracy *100 :.2f}%')

pred = log_reg.predict(x_test)
cm = confusion_matrix(y_test,pred)
sns.heatmap(cm, annot=True, fmt='d',cmap='Blues')
plt.title('Confusion matrix')
plt.xlabel('Predcted lablel')
plt.ylabel('True lable')
plt.show()

print(classification_report(y_test,pred))

"""### 2- KNN"""

knn = KNeighborsClassifier()
knn.fit(x_train, y_train)

knn_accuracy = accuracy_score(y_test,knn.predict(x_test))
print(f'Accuracy: {knn_accuracy *100 :.2f}%')

pred = knn.predict(x_test)
cm = confusion_matrix(y_test,pred)
sns.heatmap(cm, annot=True, fmt='d',cmap='Blues')
plt.title('Confusion matrix')
plt.xlabel('Predcted lablel')
plt.ylabel('True lable')
plt.show()

print(classification_report(y_test,pred))

"""
### 3- Random Forest"""

rand_clf = RandomForestClassifier(criterion = 'entropy', max_depth = 11,
                                  min_samples_leaf = 2, min_samples_split = 3, n_estimators = 130)
rand_clf.fit(x_train, y_train)

rand_accuracy = accuracy_score(y_test,rand_clf.predict(x_test))
print(f'Accuracy: {rand_accuracy *100 :.2f}%')

pred = rand_clf.predict(x_test)
cm = confusion_matrix(y_test,pred)
sns.heatmap(cm, annot=True, fmt='d',cmap='Blues')
plt.title('Confusion matrix')
plt.xlabel('Predcted lablel')
plt.ylabel('True lable')
plt.show()

print(classification_report(y_test,pred))

"""# **Comparing Accuracies**"""

scores=[log_accuracy, knn_accuracy, rand_accuracy]
models = ['Logistic Regression', 'KNN', 'Random Forest']
scores_with_names = list(zip(models, scores))
sorted_scores_with_names = sorted(scores_with_names, key=lambda x: x[1], reverse=True)
print("Scores from highest to smallest accuracy:")
for rank, (name, score) in enumerate(sorted_scores_with_names, start=1):
    print(f"{rank}- {name}: {score *100 :.2f}%")

def plot_comparison_barplot(models, scores):

    sns.set(style="whitegrid")
    plt.figure(figsize=(10, 6))
    bars = plt.bar(models, [score * 100 for score in scores], color='skyblue', edgecolor='black', linewidth=1)

    for bar, score in zip(bars, scores):
        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1.5,
                 f"{score * 100:.1f}%", ha='center', va='bottom', fontsize=11)

    max_index = scores.index(max(scores))
    bars[max_index].set_color('dodgerblue')
    bars[max_index].set_edgecolor('navy')
    plt.text(bars[max_index].get_x() + bars[max_index].get_width() / 2,
             bars[max_index].get_height() + 10,
             "Best Model", ha='center', va='bottom', fontsize=12, color='darkred', fontweight='bold')

    plt.xlabel('Models', fontsize=14)
    plt.ylabel('Accuracy (%)', fontsize=14)
    plt.title('Comparison of Model Accuracies', fontsize=16, fontweight='bold')
    plt.ylim(0, 130)

    plt.grid(axis='y', linestyle='--', alpha=0.7)

    plt.tight_layout()
    plt.show()


plot_comparison_barplot(models, scores)